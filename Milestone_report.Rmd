---
title: "Capstone Project Milestone Report"
author: "ThuyLe"
date: "5/7/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Executive Summary
This milestone report shows work in week 2 of Capstone project Course on the Data Science Specialization by John Hopkins University on Coursera. 
The ultimost goal of capstone project is to predict the next word for a string, conducted on a shiny app. This report aims to explain the explortary analysis and prepare for the eventual app and algorithm.
## Loading data and exploratory analysis
### Loading data and packages
#### Data is loaded and read by readLines function
```{r, warning=FALSE}
en_blogs = readLines("/Users/thuyle/Documents/Documents_Thuy’sMacBookAir/Data science course/Capstone_project/data/en_US/en_US.blogs.txt")
en_news = readLines("/Users/thuyle/Documents/Documents_Thuy’sMacBookAir/Data science course/Capstone_project/data/en_US/en_US.news.txt")
en_twitter = readLines("/Users/thuyle/Documents/Documents_Thuy’sMacBookAir/Data science course/Capstone_project/data/en_US/en_US.twitter.txt")
```
#### I used quanteda package for natural language process (building corpus, cleaning and tokenizing the corpus as well as building ngram). Some other packages like tidyr, dplyr, data.table are used to clean the data and build the table for ngram. 

```{r, message=FALSE, warning=FALSE}
library(quanteda)
library(tm)
library(tidyr)
library(dplyr)
library(ngram)
library(data.table)
library(stringr)
library(wordcloud)
```

### Exploratory analysis
#### What is the length of the longest line seen in any of the three en_US data sets? 
```{r, warning=FALSE}
nchar_blogs <- sapply(en_blogs, nchar)
nchar_news <- sapply(en_news, nchar)
nchar_twitter <- sapply(en_twitter, nchar)
max_nchar <- max(nchar_blogs, nchar_news, nchar_twitter)
```

#### In the en_US twitter data set, if you divide the number of lines where the word "love" (all lowercase) occurs by the number of lines the word "hate" (all lowercase) occurs, about what do you get?
```{r, warning=FALSE}
length(grep("love", en_twitter))/length(grep("hate", en_twitter))
```

#### The one tweet in the en_US twitter data set that matches the word "biostats" says what?
```{r, warning=FALSE}
grep("biostats", en_twitter, value=TRUE)
```

#### How many tweets have the exact characters "A computer once beat me at chess, but it was no match for me at kickboxing". (I.e. the line matches those characters exactly.)
```{r, warning=FALSE}
length(grep("A computer once beat me at chess, but it was no match for me at kickboxing", en_twitter, value=TRUE))
```

### Sampling the data
#### The total data is subset by 30% to do cleaning and further analysis
```{r, warning=FALSE}
samples_blogs <- en_blogs[rbinom(length(en_blogs), 1, 0.3) ==1] 
samples_news <- en_news[rbinom(length(en_blogs), 1, 0.3) ==1]
samples_twitter <- en_twitter[rbinom(length(en_blogs), 1, 0.3) ==1]
stri_stats_general(samples_blogs) # số dòng, số ký tự
stri_stats_latex (samples_blogs) # số từ số ký tự
stri_count_words(samples_blogs) # tính số từ trong mỗi dòng
data <- c(samples_blogs, samples_news, samples_twitter)
```


#### Tokenize and cleaning the texts with quanteda package:
####- Transfer all text to lowercase
####- Split hyphens
####- Remove seperators
####- Removing punctuation 
####- Removing url
####- Remove symbols
####- Removing number
####- Remove the most common words of the language (stop words)
####- Remove all other characters that are not letters and all the words with 2 or more consecutive repeated characters

```{r, warning=FALSE}
tokendata <- function(data) {
  data <- tokens(data)
  data <- tokens_tolower(data)
  data <- tokens(data, split_hyphens = TRUE)
  data <- tokens(data, remove_separators = TRUE)
  data <- tokens(data, remove_punct = TRUE)
  data <- tokens(data, remove_url = TRUE)
  data <- tokens(data, remove_symbols = TRUE)
  data <- tokens(data, remove_numbers = TRUE)
  data <- tokens_remove(data, stopwords("en"))
 data <- tokens_select(data, c("[^a-zA-Z]", "([a-zA-Z0-9])\\1\\1+"), selection = "remove", valuetype = "regex")
  return(data)
}
dt <- tokendata(data)
```

### Building ngram
### Unigram
#### We shall use tokens_grams function with n = 1 to build unigram. Then building the document feature matrix. The matrix shows the count of each word by line of text (coresponding to each column). Therefore, we have to summarize by column to obtain the total count of each word. Afterward turning document features matrix to a table for further process.
```{r, warning=FALSE}
dt <- corpus(data)
dt <- tokendata(data)
ngram <- tokens_ngrams(dt, n=1)
dfm <- dfm(ngram)
dfm <- dfm_trim(dfm, min_termfreq = 10)
dfmS <- colSums(dfm)
unigram <- data.table(word1=names(dfmS), freq=dfmS)
unigram <- arrange(unigram, -freq)
setkey(unigram, word1)
head(unigram)
```
### Bigram and Trigram
#### Simmilarly, we get bigram and trigram, just have to change n to 2 and 3 respectively in tokens_gram function. The difference is to split the 2-word strings to 2 words on 2 columns as well as split 3-word strings to 3 words into 3 columns in the table.
```{r, warning=FALSE}
ngram2 <- tokens_ngrams(dt, n=2)
dfm2 <- dfm(ngram2)
dfm2 <- dfm_trim(dfm2, min_termfreq = 20)
dfm2S <- colSums(dfm2)
bigram <- data.table(
  word1 = sapply(strsplit(names(dfm2S), "_", fixed = TRUE), "[[", 1),
  word2 = sapply(strsplit(names(dfm2S), "_", fixed = TRUE), "[[", 2),
  freq = dfm2S
)
setkey(bigram, word1, word2)
bigram <- arrange(bigram, -freq)

discount_value <- 0.75
head(bigram)
trigram <- tokens_ngrams(dt, n=3)
dfm3 <- dfm(trigram)
dfm3S <- colSums(dfm3)
trigram <- data.table(
  word1 = sapply(strsplit(names(dfm3S), "_", fixed = TRUE), "[[", 1),
  word2 = sapply(strsplit(names(dfm3S), "_", fixed = TRUE), "[[", 2),
  word3 = sapply(strsplit(names(dfm3S), "_", fixed = TRUE), "[[", 3),
  freq = dfm3S
)
trigram <- arrange(trigram, -freq)
setkey(trigram, word1, word2, word3)
head(trigram)
```
### Visualizing the ngram with word cloud
```{r, warning=FALSE}
set.seed(1234)
unicloud <-  wordcloud(words = unigram$word1, freq = unigram$freq, min.freq = 1,
            max.words=200, random.order=FALSE, rot.per=0.35, 
            colors=brewer.pal(8, "Dark2"))  
bicloud <-  wordcloud(words = paste(bigram$word1, bigram$word2), freq = bigram$freq, min.freq = 1,
                       max.words=200, random.order=FALSE, rot.per=0.35, 
                       colors=brewer.pal(8, "Dark2")) 
trigram <- wordcloud(words = paste(trigram$word1, trigram$word2, trigram$word3), freq = trigram$freq, min.freq = 1,
                     max.words=200, random.order=FALSE, rot.per=0.35, 
                     colors=brewer.pal(8, "Dark2")) 
```

## The goal for eventual app and algorithm
#### The next step we shall build the algorithm of prediction model based on Kneser Ney Smoothing. This is a method primarily used to calculate the probability distribution of n-grams in a document based on their histories. The next word is suggested based on not only it's probability but also the words preceding it. 
#### We shall build 3 functions. The first one randomly return one of five words having highest probability. The second one return the word with highest probability to go with one preceding word. The last one return the highest frequent word to appear after two preceding words.Then we build the function to predict the next word when we type a sequence of words. It will take the guess from trigram first. If there is no word found, it will work in bigram, then unigram. 



